# -*- coding: utf-8 -*-
"""KM_chatbot

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GO0F3MHuFta4FGzCWsG_sV0FoJTzF3-D
"""

!pip install llama-cpp-python langchain PyPDF2 sentence-transformers faiss-cpu

"""upload multiple PDFs"""

import os
import shutil
from google.colab import files

# Step 1: Define folder to store uploaded PDFs
pdf_dir = "/content/pdfs"

# Step 2: Remove old PDF directory and recreate it
if os.path.exists(pdf_dir):
    shutil.rmtree(pdf_dir)
os.makedirs(pdf_dir)

# Step 3: Delete any previously uploaded PDFs from root directory
for f in os.listdir():
    if f.endswith(".pdf"):
        os.remove(f)

# Step 4: Upload new PDFs
uploaded = files.upload()  # Upload new files using the UI
pdf_files = list(uploaded.keys())
print("üìÑ Uploaded PDFs:", pdf_files)

# Step 5: Move uploaded files to the clean directory
for file_name in pdf_files:
    os.rename(file_name, os.path.join(pdf_dir, file_name))

from PyPDF2 import PdfReader
from langchain.schema import Document
import os

def extract_documents_from_pdfs(pdf_files):
    documents = []
    for file_path in pdf_files:
        reader = PdfReader(file_path)
        filename = os.path.basename(file_path)
        for i, page in enumerate(reader.pages):
            text = page.extract_text()
            if text:
                documents.append(Document(
                    page_content=text,
                    metadata={"source": filename, "page": i + 1}
                ))
    return documents

# Full path generation
pdf_dir = "/content/pdfs"  # Adjust if different
pdf_paths = [os.path.join(pdf_dir, fname) for fname in pdf_files]

# Extract documents
documents = extract_documents_from_pdfs(pdf_paths)
print("‚úÖ Extracted", len(documents), "document pages with metadata")

from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=200,
    separators=["\n\n", "\n", ".", " ", ""]
)

chunked_docs = splitter.split_documents(documents)
print(f"üîç Created {len(chunked_docs)} chunks")

from langchain.embeddings import SentenceTransformerEmbeddings
from langchain.vectorstores import FAISS

embedder = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")

if "vectorstore" not in globals():
    print("‚öôÔ∏è Generating embeddings and building vectorstore...")
    vectorstore = FAISS.from_documents(chunked_docs, embedder)
    print(f"‚úÖ Vectorstore created with {len(chunked_docs)} chunks.")
else:
    print("‚úÖ Reusing existing vectorstore.")
print("üß† Vectorstore ID:", id(vectorstore))

from huggingface_hub import hf_hub_download
from llama_cpp import Llama

model_path = hf_hub_download(
    repo_id="TheBloke/TinyLLaMA-1.1B-Chat-v1.0-GGUF",
    filename="tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
    cache_dir="models"
)

llm = Llama(model_path=model_path, n_ctx=2048)

def retrieve_context(query, k=3):
    """
    Retrieves the top-k most relevant document chunks using pure semantic retrieval.
    Returns the assembled context and a list of source references.
    """
    docs = retriever.get_relevant_documents(query)

    # Simply take the top-k results, unfiltered
    top_docs = docs[:k]

    context = "\n\n".join(doc.page_content.strip() for doc in top_docs)
    sources = [
        f"‚Ä¢ {doc.metadata.get('source', 'Unknown')} (page {doc.metadata.get('page', '?')})"
        for doc in top_docs
    ]

    return context, sources

def ask_pdf_bot(query):
    context, sources = retrieve_context_with_sources(query)

    prompt = f"""<|system|>
You are an expert regulatory assistant for Indian food safety laws (FSSAI). Answer the user's question using only the information in the context. Extract numeric standards, limits, or named regulations whenever possible. Use bullet points to structure the answer.
<|user|>
Context:
{context}

Question: {query}
<|assistant|>"""

    print("üßæ Full prompt (truncated):")
    print(prompt[:500] + "..." if len(prompt) > 500 else prompt)

    response = llm(
    prompt,
    max_tokens=300,
    temperature=0.0,  # Deterministic response
    top_p=1.0,
    seed=42
)
    answer = response['choices'][0]['text'].strip()
    source_info = "\n\nüìÑ Sources:\n" + "\n".join(f"‚Ä¢ {src}" for src in sources)
    return answer + source_info

def compare_outputs(query):
    print("\nüîé Testing with query:", repr(query))  # ‚úÖ add this
    print("\nüìå Notebook Output:")
    notebook_answer = ask_pdf_bot(query)

    print("\nüìå Gradio Output:")
    gradio_answer = gradio_chat([{"role": "user", "content": query}])

    if notebook_answer.strip() == gradio_answer.strip():
        print("\n‚úÖ SAME output")
    else:
        print("\n‚ùå DIFFERENT output")
        print("\nüìù Notebook:\n", notebook_answer)
        print("\nüßë‚Äçüíª Gradio:\n", gradio_answer)

import gradio as gr
import logging

# ‚úÖ Logging config
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)

def gradio_chat(messages, state=None):
    logging.info(f"üì• Raw Gradio messages: {messages}")

    question = ""

    # ‚úÖ Handle Gradio message formats (dict, tuple, or string)
    if isinstance(messages, list):
        last = messages[-1]
        if isinstance(last, dict) and "content" in last:
            question = last["content"]
        elif isinstance(last, tuple) and len(last) == 2:
            question = last[1]
        else:
            question = str(last)
    elif isinstance(messages, str):
        # üî• Direct string input (Gradio fallback)
        question = messages
    else:
        logging.warning("‚ùå Unknown input type.")
        return "‚ö†Ô∏è Unexpected input format."

    question = question.strip()
    logging.info(f"ü§ñ Gradio Question: {repr(question)}")

    if not question or question == "?":
        return "‚ö†Ô∏è Please ask a more specific question than just '?'"

    return ask_pdf_bot(question)

# ‚úÖ Define UI
chatbot_ui = gr.ChatInterface(
    fn=gradio_chat,
    chatbot=gr.Chatbot(height=200),  # üõ†Ô∏è Removed type="messages"
    textbox=gr.Textbox(placeholder="Ask a question about the FSSAI standards...", scale=7),
    title="üìòFSSAI KM Chatbot",
    description="Ask a question about the FSSAI standards",
    theme="soft",
    cache_examples=False,
    concurrency_limit=1,
    # üõ†Ô∏è Removed type="messages"
)

chatbot_ui.launch(share=True, debug=True)